{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "import sys \n",
    "import scipy \n",
    "import pickle \n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.model_selection import train_test_split \n",
    "sys.path.append('/Users/lei/home/studyhall/smart-earth-sensing/office-demo-project/lib') \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "import torch \n",
    "from torch.nn.functional import one_hot \n",
    " \n",
    "from spec import * \n",
    "from preprocessing import *\n",
    "from h5reader import * \n",
    "from audio_augmentation import * \n",
    "from dataset import Audio_Dataset \n",
    "from visualize import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/lei/home/studyhall/smart-earth-sensing/office-demo-project/data/walk_run_spec.txt', 'r') as f:\n",
    "#     specs = [Spec(re.sub(\"\\n| \", \"\", line).split(',')) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_path = '/Users/lei/home/studyhall/smart-earth-sensing/office-demo-project/data/corpus'\n",
    "# feature_set = []\n",
    "# label_set = []\n",
    "# for spec in specs:\n",
    "#     print(spec) \n",
    "#     for time in spec.duration: \n",
    "#         for path in os.listdir(dir_path):\n",
    "#             if path[42:46] == time:\n",
    "#                 raw, _, _ = read_data(dir_path + '/' + path, [20], [21]) \n",
    "#                 samples4halfsec = list(divide_chunks(raw, 500))\n",
    "#                 feature_set += samples4halfsec\n",
    "#                 for i in range(0, np.shape(samples4halfsec)[0]):\n",
    "#                     label_set.append(spec.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sr = 1000 \n",
    "# augmented_featrue_set = []\n",
    "# augmented_label_set = [] \n",
    "# for i in tqdm(range(0, len(feature_set))):\n",
    "#     audio = feature_set[i] \n",
    "#     augmented_featrue_set += [audio, \n",
    "#                             shift_pitch(audio=audio, sr=sr, pitch_factor=1.5), \n",
    "#                             shift_time(audio=audio, shift_max=50, shift_direction='rand'), \n",
    "#                             noise_injection(audio=audio, noise_factor=0.2)] \n",
    "#     augmented_label_set += [label_set[i] for j in range(0, 4)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented featrure shape: (31200, 500, 1)\n",
      "respective label shape: (31200,)\n"
     ]
    }
   ],
   "source": [
    "augmented_feature_set = []\n",
    "augmented_label_set = [] \n",
    "with open('../pk_files/augmented_corpus.pkl', 'rb') as f: \n",
    "    (augmented_feature_set, augmented_label_set) = pickle.load(f)\n",
    "     \n",
    "print(\"augmented featrure shape: {}\".format(np.shape(augmented_feature_set)))\n",
    "print(\"respective label shape: {}\".format(np.shape(augmented_label_set))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31200/31200 [02:27<00:00, 211.14it/s]\n"
     ]
    }
   ],
   "source": [
    "features = np.reshape(np.array([raw2mfcc(raw=raw) for raw in tqdm(augmented_feature_set)]), (31200, 1, 26, 64))\n",
    "labels = np.array(augmented_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.3)\n",
    "X_train = [torch.tensor(sample, dtype=torch.float32) for sample in X_train]\n",
    "X_val = [torch.tensor(sample, dtype=torch.float32) for sample in X_val]\n",
    "y_train = [one_hot(torch.tensor(sample, dtype=torch.long), num_classes=3).to(torch.float32) for sample in y_train]\n",
    "y_val = [one_hot(torch.tensor(sample, dtype=torch.long), num_classes=3).to(torch.float32) for sample in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = Audio_Dataset(X_train, y_train, len(y_train))\n",
    "data_val = Audio_Dataset(X_val, y_val, len(y_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/train_val_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump((data_train, data_val), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smart-earth-sensing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
